# Production Optimization Strategies

## Critical Issues with Current Design

### ‚ùå **Network Security Problems**
**Current Issue**: All nodes have public IPs and direct internet access
**Production Risk**: Massive security vulnerability - exposes entire cluster to internet

### ‚ùå **Image Management Problems**  
**Current Issue**: Every deployment downloads all images from scratch
**Production Impact**: 400-600MB per node √ó 5 nodes √ó multiple deployments = 1GB+ data transfer

---

## üèóÔ∏è **Network Architecture Optimization**

### **Private Subnet Architecture**
```hcl
# Private subnets for worker nodes - NO public IPs
resource "aws_subnet" "k8s_private" {
  count                   = 2
  vpc_id                  = aws_vpc.k8s_vpc.id
  cidr_block              = "10.0.${count.index + 10}.0/24"
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = false  # CRITICAL: No public IPs

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-private-subnet-${count.index + 1}"
    Type = "Private"
    "kubernetes.io/role/internal-elb" = "1"
  })
}

# NAT Gateway for outbound internet (worker nodes need package downloads)
resource "aws_eip" "nat_eip" {
  domain = "vpc"
  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-nat-eip"
  })
}

resource "aws_nat_gateway" "k8s_nat" {
  allocation_id = aws_eip.nat_eip.id
  subnet_id     = aws_subnet.k8s_public.id  # NAT goes in public subnet
  
  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-nat-gateway"
  })
}

# Private route table - routes through NAT Gateway
resource "aws_route_table" "k8s_private_rt" {
  vpc_id = aws_vpc.k8s_vpc.id

  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.k8s_nat.id  # Route through NAT, not IGW
  }

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-private-rt"
  })
}

resource "aws_route_table_association" "k8s_private_rta" {
  count          = length(aws_subnet.k8s_private)
  subnet_id      = aws_subnet.k8s_private[count.index].id
  route_table_id = aws_route_table.k8s_private_rt.id
}
```

### **Bastion Host for Admin Access**
```hcl
# Only control plane needs public IP for kubectl access
resource "aws_instance" "k8s_control_plane" {
  ami                    = data.aws_ami.ubuntu.id
  instance_type          = "t2.micro"
  subnet_id              = aws_subnet.k8s_public.id  # Public subnet
  # ... existing config
}

# Worker nodes in private subnets - NO public IPs
resource "aws_instance" "k8s_workers" {
  count                  = 4
  ami                    = data.aws_ami.ubuntu.id
  instance_type          = "t2.micro"
  subnet_id              = aws_subnet.k8s_private[count.index % 2].id  # Private subnets
  # ... existing config, but NO map_public_ip_on_launch
}
```

### **External Access Only for Home Assistant**
```yaml
# ingress.yaml - Only Home Assistant gets external access
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: homeassistant-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - plant.yourdomain.com
    secretName: homeassistant-tls
  rules:
  - host: plant.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: homeassistant
            port:
              number: 8123

---
# Network policy - restrict access to Home Assistant
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: homeassistant-access
spec:
  podSelector:
    matchLabels:
      app: homeassistant
  policyTypes:
  - Ingress
  ingress:
  - from: []  # Allow from anywhere (external users)
    ports:
    - protocol: TCP
      port: 8123
  - from:  # Allow from processor for data
    - podSelector:
        matchLabels:
          app: plant-processor
    ports:
    - protocol: TCP
      port: 8123

---
# Internal services - NO external access
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-services-policy
spec:
  podSelector:
    matchLabels:
      tier: internal  # MongoDB, Kafka, Processor
  policyTypes:
  - Ingress
  ingress:
  - from:  # Only from within cluster
    - namespaceSelector: {}
```

---

## üì¶ **Container Image Optimization**

### **1. Container Registry Caching**
```hcl
# ECR Private Registry for image caching
resource "aws_ecr_repository" "plant_monitoring_images" {
  for_each = toset([
    "homeassistant",
    "mongodb", 
    "kafka",
    "plant-processor",
    "plant-sensor"
  ])
  
  name                 = "plant-monitoring/${each.key}"
  image_tag_mutability = "MUTABLE"
  
  image_scanning_configuration {
    scan_on_push = true
  }
  
  lifecycle_policy {
    policy = jsonencode({
      rules = [{
        rulePriority = 1
        description  = "Keep last 5 images"
        selection = {
          tagStatus     = "tagged"
          tagPrefixList = ["v"]
          countType     = "imageCountMoreThan"
          countNumber   = 5
        }
        action = {
          type = "expire"
        }
      }]
    })
  }
}

# ECR endpoints for VPC - reduces data transfer costs
resource "aws_vpc_endpoint" "ecr_dkr" {
  vpc_id              = aws_vpc.k8s_vpc.id
  service_name        = "com.amazonaws.${var.aws_region}.ecr.dkr"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.k8s_private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints.id]
  
  private_dns_enabled = true
  
  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-ecr-dkr-endpoint"
  })
}

resource "aws_vpc_endpoint" "ecr_api" {
  vpc_id              = aws_vpc.k8s_vpc.id
  service_name        = "com.amazonaws.${var.aws_region}.ecr.api"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = aws_subnet.k8s_private[*].id
  security_group_ids  = [aws_security_group.vpc_endpoints.id]
  
  private_dns_enabled = true
  
  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-ecr-api-endpoint"
  })
}
```

### **2. Build and Push Optimized Images**
```bash
#!/bin/bash
# build-and-cache-images.sh

ECR_REGISTRY="123456789012.dkr.ecr.us-east-2.amazonaws.com"
REGION="us-east-2"

# Login to ECR
aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ECR_REGISTRY

# Build minimal images with multi-stage builds
build_image() {
    local app=$1
    local version=$2
    
    echo "Building optimized $app image..."
    
    # Build with BuildKit optimizations
    DOCKER_BUILDKIT=1 docker build \
        --target production \
        --build-arg VERSION=$version \
        --tag $ECR_REGISTRY/plant-monitoring/$app:$version \
        --tag $ECR_REGISTRY/plant-monitoring/$app:latest \
        ./applications/$app/
    
    # Push to ECR
    docker push $ECR_REGISTRY/plant-monitoring/$app:$version
    docker push $ECR_REGISTRY/plant-monitoring/$app:latest
    
    echo "‚úÖ $app:$version pushed to ECR"
}

# Build all images
build_image "plant-processor" "v1.0.0"
build_image "plant-sensor" "v1.0.0"

# Pull and re-tag existing optimized images
docker pull homeassistant/home-assistant:2024.10.1
docker tag homeassistant/home-assistant:2024.10.1 $ECR_REGISTRY/plant-monitoring/homeassistant:2024.10.1
docker push $ECR_REGISTRY/plant-monitoring/homeassistant:2024.10.1

docker pull mongo:7.0.14
docker tag mongo:7.0.14 $ECR_REGISTRY/plant-monitoring/mongodb:7.0.14  
docker push $ECR_REGISTRY/plant-monitoring/mongodb:7.0.14

docker pull confluentinc/cp-kafka:7.4.4
docker tag confluentinc/cp-kafka:7.4.4 $ECR_REGISTRY/plant-monitoring/kafka:7.4.4
docker push $ECR_REGISTRY/plant-monitoring/kafka:7.4.4

echo "üéâ All images cached in ECR"
```

### **3. Optimized Dockerfile Examples**
```dockerfile
# applications/plant-processor/Dockerfile
FROM node:18-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

FROM node:18-alpine AS production
RUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001
WORKDIR /app
COPY --from=builder --chown=nodejs:nodejs /app/node_modules ./node_modules
COPY --chown=nodejs:nodejs . .
USER nodejs
EXPOSE 3000
CMD ["node", "server.js"]

# Result: 50MB instead of 200MB
```

### **4. DaemonSet for Image Pre-pulling**
```yaml
# image-prepull-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: image-prepull
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: image-prepull
  template:
    metadata:
      labels:
        app: image-prepull
    spec:
      initContainers:
      - name: prepull-images
        image: alpine:3.18
        command: 
        - /bin/sh
        - -c
        - |
          # Pre-pull all application images to each node
          echo "Pre-pulling images on $(hostname)..."
          
          # This runs on each node and pulls images to local Docker cache
          apk add --no-cache docker
          
          # List of images to pre-pull
          IMAGES=(
            "123456789012.dkr.ecr.us-east-2.amazonaws.com/plant-monitoring/homeassistant:2024.10.1"
            "123456789012.dkr.ecr.us-east-2.amazonaws.com/plant-monitoring/mongodb:7.0.14"
            "123456789012.dkr.ecr.us-east-2.amazonaws.com/plant-monitoring/kafka:7.4.4"
            "123456789012.dkr.ecr.us-east-2.amazonaws.com/plant-monitoring/plant-processor:v1.0.0"
            "123456789012.dkr.ecr.us-east-2.amazonaws.com/plant-monitoring/plant-sensor:v1.0.0"
          )
          
          for image in "${IMAGES[@]}"; do
            echo "Pulling $image..."
            docker pull $image || echo "‚ö†Ô∏è Failed to pull $image"
          done
          
          echo "‚úÖ Image pre-pull complete on $(hostname)"
        volumeMounts:
        - name: docker-sock
          mountPath: /var/run/docker.sock
        securityContext:
          privileged: true
      containers:
      - name: sleep
        image: alpine:3.18
        command: ["sleep", "infinity"]
        resources:
          requests:
            cpu: 10m
            memory: 16Mi
          limits:
            cpu: 10m
            memory: 16Mi
      volumes:
      - name: docker-sock
        hostPath:
          path: /var/run/docker.sock
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
      - operator: Exists
```

---

## üîÑ **Image Update Strategy**

### **Production Deployment Pipeline**
```yaml
# .github/workflows/production-deploy.yml
name: Production Deploy with Image Optimization
on:
  push:
    branches: [main]
    paths: ['applications/**']

jobs:
  build-and-cache:
    runs-on: ubuntu-latest
    outputs:
      image-tags: ${{ steps.build.outputs.tags }}
    steps:
    - uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-2
    
    - name: Login to ECR
      uses: aws-actions/amazon-ecr-login@v2
    
    - name: Build and push optimized images
      id: build
      run: |
        # Only build changed applications
        CHANGED_APPS=$(git diff --name-only HEAD~1 HEAD | grep "applications/" | cut -d'/' -f2 | sort -u)
        
        for app in $CHANGED_APPS; do
          echo "Building $app..."
          
          # Build with cache-from for faster builds
          docker buildx build \
            --platform linux/amd64 \
            --cache-from type=registry,ref=$ECR_REGISTRY/plant-monitoring/$app:cache \
            --cache-to type=registry,ref=$ECR_REGISTRY/plant-monitoring/$app:cache,mode=max \
            --push \
            --tag $ECR_REGISTRY/plant-monitoring/$app:$GITHUB_SHA \
            --tag $ECR_REGISTRY/plant-monitoring/$app:latest \
            ./applications/$app/
        done

  rolling-update:
    needs: build-and-cache
    runs-on: ubuntu-latest
    steps:
    - name: Deploy to cluster
      run: |
        # Rolling update with zero downtime
        kubectl set image deployment/homeassistant \
          homeassistant=$ECR_REGISTRY/plant-monitoring/homeassistant:$GITHUB_SHA
        
        kubectl rollout status deployment/homeassistant --timeout=300s
        
        # If rollout fails, automatic rollback
        if [ $? -ne 0 ]; then
          kubectl rollout undo deployment/homeassistant
          exit 1
        fi
```

---

## üìä **Data Transfer Savings**

### **Current (Problematic) Approach**
```
First Deployment:
- Home Assistant: 400MB √ó 5 nodes = 2.0GB
- MongoDB: 150MB √ó 5 nodes = 750MB  
- Kafka: 200MB √ó 5 nodes = 1.0GB
- System images: 200MB √ó 5 nodes = 1.0GB
Total: ~4.75GB per deployment

Multiple deployments = 10GB+ easily! üí∏
```

### **Optimized Approach**
```
ECR + VPC Endpoints:
- Initial cache population: 1GB total (one-time)
- Subsequent deployments: ~50MB (only changed images)
- VPC endpoints eliminate internet data transfer costs
- Image layers shared between containers

Savings: 90%+ reduction in data transfer! üí∞
```

---

## üõ°Ô∏è **Security Benefits Summary**

| Component | Current Risk | Production Solution |
|-----------|-------------|-------------------|
| **Worker Nodes** | Public IPs = Internet accessible | Private subnets = Internal only |
| **Database** | Exposed to internet | Network policies = Cluster only |
| **Kafka** | Public access possible | Internal LB = App access only |
| **Home Assistant** | Mixed with internal services | Dedicated ingress = Controlled external access |
| **SSH Access** | Direct to all nodes | Bastion host = Single entry point |

---

## üí° **Implementation Phases**

### **Phase 1: Network Security** (Critical - Week 1)
1. Implement private subnets for worker nodes
2. Add NAT Gateway for outbound access
3. Configure ingress only for Home Assistant
4. Implement network policies

### **Phase 2: Image Optimization** (High Impact - Week 2)  
1. Set up ECR private registry
2. Add VPC endpoints for ECR
3. Build optimized multi-stage Dockerfiles
4. Implement image pre-pulling

### **Phase 3: Automation** (Long-term - Week 3+)
1. CI/CD pipeline with image caching
2. Automated security scanning
3. Image vulnerability management
4. Cost monitoring and alerts

This approach transforms your architecture from a "learning demo" to a **production-ready, secure, and cost-optimized** system! üöÄ